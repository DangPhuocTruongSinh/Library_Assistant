import uuid
from typing import List, Dict, Any

from langchain_core.documents import Document

from database.connection import (
    chroma_client, 
    embedding_model, 
    LangchainEmbeddingFunction
)

from log.logger_config import setup_logging
logger = setup_logging(__name__)

class ReferenceRetrievalSystem:
    """
    H·ªá th·ªëng t√¨m ki·∫øm s·ª≠ d·ª•ng:
    1. Chroma Cloud cho Vector Search (persistent, scalable)
    2. Same-Page Context Expansion (l·∫•y T·∫§T C·∫¢ chunks c√πng trang v·ªõi k·∫øt qu·∫£)
    
    Kh√¥ng m·ªü r·ªông sang trang kh√°c ƒë·ªÉ tr√°nh chi·∫øm d·ª•ng context window
    v·ªõi n·ªôi dung kh√¥ng li√™n quan.
    """
    
    def __init__(self, collection_name: str = "PDF_Reader"):
        """
        Kh·ªüi t·∫°o h·ªá th·ªëng t√¨m ki·∫øm.
        
        Args:
            collection_name: T√™n collection trong Chroma Cloud.
                            M·ªói phi√™n ƒë·ªçc s√°ch c√≥ th·ªÉ d√πng collection ri√™ng.
        """
        self.collection_name = collection_name
        self.client = chroma_client
        self.embedding_func = LangchainEmbeddingFunction(embedding_model)
        self.collection = None
        
        self._init_collection()

    def _init_collection(self):
        """Kh·ªüi t·∫°o collection - gi·ªØ l·∫°i d·ªØ li·ªáu n·∫øu ƒë√£ t·ªìn t·∫°i."""
        try:
            # Th·ª≠ l·∫•y collection ƒë√£ t·ªìn t·∫°i (kh√¥ng truy·ªÅn embedding_function ƒë·ªÉ tr√°nh conflict)
            self.collection = self.client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_func
            )
            logger.info(f"‚úÖ ƒê√£ k·∫øt n·ªëi collection '{self.collection_name}' (ƒë√£ t·ªìn t·∫°i).")
        except Exception:
            # Ch∆∞a t·ªìn t·∫°i ‚Üí t·∫°o m·ªõi
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_func
            )
            logger.info(f"‚úÖ ƒê√£ t·∫°o collection m·ªõi '{self.collection_name}'.")

    def clear_collection(self):
        """
        X√≥a to√†n b·ªô d·ªØ li·ªáu trong collection (d√πng khi user m·ªü s√°ch m·ªõi).
        """
        if self.collection:
            try:
                self.client.delete_collection(name=self.collection_name)
                self._init_collection()
                logger.info(f"üóëÔ∏è ƒê√£ x√≥a v√† t·∫°o l·∫°i collection '{self.collection_name}'.")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è L·ªói khi x√≥a collection: {e}")

    def index_documents(self, documents: List[Document], batch_size: int = 100):
        """
        Index danh s√°ch t√†i li·ªáu v√†o Chroma Cloud s·ª≠ d·ª•ng Multi-threading ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô.
        """
        import time
        from concurrent.futures import ThreadPoolExecutor, as_completed

        total = len(documents)
        total_batches = (total + batch_size - 1) // batch_size
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu Multi-threaded Indexing: {total} chunks, {total_batches} batches...")

        # Chia documents th√†nh c√°c batch
        batches = []
        for i in range(0, total, batch_size):
            batches.append(documents[i : i + batch_size])

        def process_batch(batch_idx, batch_data):
            current_batch = batch_idx + 1
            ids = [str(uuid.uuid4()) for _ in range(len(batch_data))]
            docs = [doc.page_content for doc in batch_data]
            metadatas = [doc.metadata for doc in batch_data]
            
            max_retries = 3
            retry_count = 0
            backoff_time = 15 # TƒÉng base backoff l√™n m·ªôt ch√∫t cho multi-threading

            while retry_count <= max_retries:
                try:
                    self.collection.upsert(
                        ids=ids,
                        documents=docs,
                        metadatas=metadatas
                    )
                    return f"‚úÖ Batch {current_batch} OK"
                except Exception as e:
                    error_msg = str(e).lower()
                    if "429" in error_msg or "quota" in error_msg or "limit" in error_msg:
                        retry_count += 1
                        if retry_count > max_retries:
                            return f"‚ùå Batch {current_batch} FAILED after {max_retries} retries"
                        
                        wait_time = backoff_time * (2 ** (retry_count - 1))
                        logger.warning(f"‚ùÑÔ∏è Thread-Batch {current_batch}: Rate limit. Retry in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        return f"‚ùå Batch {current_batch} ERROR: {e}"

        # Th·ª±c thi multi-threading
        # Gi·∫£m xu·ªëng 2 workers ƒë·ªÉ tr√°nh ƒë√°nh nhau qu√° m·∫°nh d·∫´n ƒë·∫øn Rate Limit
        failed_results = []
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_batch = {executor.submit(process_batch, i, batch): i for i, batch in enumerate(batches)}
            
            for future in as_completed(future_to_batch):
                try:
                    result = future.result()
                    if "OK" in result:
                        logger.info(result)
                    else:
                        logger.error(result)
                        failed_results.append(result)
                except Exception as e:
                    logger.error(f"‚ùå Critical error in thread: {e}")
                    failed_results.append(str(e))

        if failed_results:
            error_summary = "; ".join(failed_results[:3])
            raise Exception(f"Indexing incomplete. {len(failed_results)} batches failed. Details: {error_summary}")

        logger.info(f"üéâ Ho√†n t·∫•t Indexing! T·ªïng: {self.collection.count()} chunks.")

    def search(
        self,
        query: str,
        top_k: int = 5,
        expand_same_page: bool = True,
        filter: Dict[str, Any] = None
    ) -> List[Document]:
        """
        T√¨m ki·∫øm v·ªõi Context Expansion (ch·ªâ l·∫•y chunks c√πng trang).
        
        Args:
            query: C√¢u h·ªèi c·ªßa user.
            top_k: S·ªë chunks ban ƒë·∫ßu c·∫ßn t√¨m.
            expand_same_page: N·∫øu True, l·∫•y th√™m t·∫•t c·∫£ chunks c√πng trang v·ªõi k·∫øt qu·∫£.
                              N·∫øu False, ch·ªâ tr·∫£ v·ªÅ top_k chunks.
            filter: Dictionary filter cho ChromaDB (vd: {"type": "heading"}).
        
        Returns:
            Danh s√°ch Documents ƒë√£ ƒë∆∞·ª£c m·ªü r·ªông context (c√πng trang).
        """
        # 1. Vector Search - T√¨m Top-K chunks
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k,
                where=filter,  # Apply filter if provided
                include=["documents", "metadatas", "distances"]
            )
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi search: {e}")
            return []
        
        # 2. Convert k·∫øt qu·∫£ th√†nh LangChain Documents
        if not results or not results.get("ids") or not results["ids"][0]:
            return []
            
        initial_docs = []
        for i in range(len(results["ids"][0])):
            doc = Document(
                page_content=results["documents"][0][i],
                metadata=results["metadatas"][0][i]
            )
            initial_docs.append(doc)
        
        # 3. Context Expansion - Ch·ªâ l·∫•y chunks c√πng trang
        if expand_same_page:
            expanded_docs = self._expand_same_page(initial_docs)
            return expanded_docs
        
        return initial_docs

    def _expand_same_page(self, chunks: List[Document]) -> List[Document]:
        """
        M·ªü r·ªông context b·∫±ng c√°ch l·∫•y T·∫§T C·∫¢ chunks c√πng trang v·ªõi k·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c.
        
        Kh√¥ng l·∫•y chunks t·ª´ trang kh√°c ƒë·ªÉ tr√°nh chi·∫øm d·ª•ng context window
        v·ªõi n·ªôi dung kh√¥ng li√™n quan.
        
        Args:
            chunks: Danh s√°ch chunks ban ƒë·∫ßu t·ª´ vector search.
        
        Returns:
            Danh s√°ch Documents ƒë√£ m·ªü r·ªông (c√πng trang), s·∫Øp x·∫øp theo th·ª© t·ª±.
        """
        # Thu th·∫≠p c√°c trang c·∫ßn l·∫•y (ch·ªâ c√°c trang c√≥ trong k·∫øt qu·∫£)
        pages_to_fetch = set()
        for doc in chunks:
            page = doc.metadata.get("page", 1)
            pages_to_fetch.add(page)
        
        # Query t·∫•t c·∫£ chunks thu·ªôc c√°c trang n√†y
        expanded_docs = []
        seen_contents = set()  # Tr√°nh duplicate
        
        for page in sorted(pages_to_fetch):
            where_filter = {"page": page}

            try:
                page_results = self.collection.get(
                    where=where_filter,
                    include=["documents", "metadatas"]
                )
                
                if page_results and page_results.get("ids"):
                    for i in range(len(page_results["ids"])):
                        content = page_results["documents"][i]
                        # Tr√°nh duplicate
                        if content not in seen_contents:
                            seen_contents.add(content)
                            expanded_docs.append(Document(
                                page_content=content,
                                metadata=page_results["metadatas"][i]
                            ))
            except Exception as e:
                logger.error(f"‚ö†Ô∏è L·ªói khi l·∫•y chunks trang {page}: {e}")
        
        # S·∫Øp x·∫øp theo s·ªë trang ƒë·ªÉ context li·ªÅn m·∫°ch
        expanded_docs.sort(key=lambda d: (d.metadata.get("page", 0), d.page_content[:50]))
        
        logger.info(f"üìñ Same-Page Expansion: {len(chunks)} chunks ‚Üí {len(expanded_docs)} chunks (pages: {sorted(pages_to_fetch)})")
        
        return expanded_docs

    def get_max_page(self) -> int:
        """L·∫•y s·ªë trang l·ªõn nh·∫•t trong t√†i li·ªáu."""
        try:
            # L·∫•y 1 b·∫£n ghi duy nh·∫•t, s·∫Øp x·∫øp theo page gi·∫£m d·∫ßn
            # V√¨ Chroma kh√¥ng h·ªó tr·ª£ order_by tr·ª±c ti·∫øp trong query, ta l·∫•y h·∫øt page r·ªìi t√¨m max
            # Ho·∫∑c ƒë∆°n gi·∫£n l√† query v·ªõi limit l·ªõn v√† l·∫•y max page t·ª´ metadata
            results = self.collection.get(include=["metadatas"])
            if results and results.get("metadatas"):
                pages = [m.get("page", 0) for m in results["metadatas"]]
                return max(pages) if pages else 1
            return 1
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y max page: {e}")
            return 1

    def get_intro_chunks(self, pages: List[int] = [1]) -> List[Document]:
        """
        L·∫•y c√°c chunks thu·ªôc c√°c trang ƒë·∫ßu ƒë·ªÉ l√†m context cho c√¢u h·ªèi t√≥m t·∫Øt.
        M·∫∑c ƒë·ªãnh ch·ªâ l·∫•y trang 1.
        """
        return self.fetch_by_metadata({"page": {"$in": pages}} if len(pages) > 1 else {"page": pages[0]})

    def fetch_by_metadata(self, where_filter: Dict[str, Any], limit: int = 20) -> List[Document]:
        """
        L·∫•y chunks d·ª±a tr√™n filter metadata ch√≠nh x√°c (kh√¥ng d√πng vector search).
        """
        fetched_docs = []
        seen_contents = set()
        
        try:
            results = self.collection.get(
                where=where_filter,
                include=["documents", "metadatas"],
                limit=limit
            )
            
            if results and results.get("ids"):
                for i in range(len(results["ids"])):
                    content = results["documents"][i]
                    if content not in seen_contents:
                        seen_contents.add(content)
                        fetched_docs.append(Document(
                            page_content=content,
                            metadata=results["metadatas"][i]
                        ))
        except Exception as e:
            logger.error(f"‚ö†Ô∏è L·ªói khi fetch by metadata {where_filter}: {e}")
            
        # Sort k·∫øt qu·∫£ ƒë·ªÉ ƒë·ªçc li·ªÅn m·∫°ch (∆∞u ti√™n page sau ƒë√≥ ƒë·∫øn n·ªôi dung)
        fetched_docs.sort(key=lambda d: (d.metadata.get("page", 0), d.metadata.get("bboxes", "")))
        
        return fetched_docs

    def get_stats(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ v·ªÅ collection hi·ªán t·∫°i."""
        return {
            "collection_name": self.collection_name,
            "total_chunks": self.collection.count(),
        }
